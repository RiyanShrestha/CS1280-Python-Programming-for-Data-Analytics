Summary of the Python Program
This Python program performs data cleaning, preprocessing, and exploratory data analysis (EDA) on the Amazon_Sale_Report.csv dataset. Below is a step-by-step explanation of what was done:

1. Import Libraries
Imported essential libraries like pandas, numpy, matplotlib.pyplot, and seaborn for data manipulation, numerical operations, and visualization.
2. Load the Dataset
The dataset Amazon_Sale_Report.csv was loaded into a DataFrame using pd.read_csv().
Basic checks like df.head(), df.info(), and df.describe() were performed to understand the structure, data types, and summary statistics of the dataset.
3. Handle Missing Values
Checked for missing values using df.isnull().sum().
Filled missing values based on the type of data:
Categorical Columns: Filled with the mode (most frequent value).
Numerical Columns: Filled with the mean (average value).
Columns handled include:
'Courier Status', 'currency', 'Amount', 'ship-city', 'ship-state', 'ship-postal-code', 'ship-country', 'promotion-ids', and 'fulfilled-by'.
4. Standardize Column Names
Standardized column names by converting them to lowercase and replacing spaces with underscores for consistency and easier access.
5. Check and Remove Duplicates
Checked for duplicate rows using df.duplicated().sum().
Removed duplicate rows using df.drop_duplicates(inplace=True).
Validated that duplicates were successfully removed.
6. Handle Outliers
Used boxplots to visualize outliers in numerical columns like 'amount' and 'ship-postal-code'.
Removed outliers using the Interquartile Range (IQR) method:
Outliers were defined as values outside the range [Q1 - 1.5*IQR, Q3 + 1.5*IQR].
7. Check for Multicollinearity
Created a correlation matrix for numerical columns using df.corr().
Visualized the correlation matrix using a heatmap (sns.heatmap()).
This step helps identify highly correlated features that may cause multicollinearity in machine learning models.
8. Replace Infinite Values and Drop Remaining NaNs
Replaced infinite values (np.inf and -np.inf) with NaN using df.replace().
Dropped any remaining missing values using df.dropna().
9. Visualize Data Distributions
Created histograms for numerical columns to visualize their distributions and identify skewness.
10. Handle Skewness
Applied a log transformation to the 'amount' column using np.log1p() to reduce skewness and normalize the data.
11. Scale Numerical Columns
Scaled numerical columns like 'amount' and 'ship-postal-code' using Min-Max Scaling to bring them into a uniform range (0 to 1).
Key Takeaways
Data Cleaning: Missing values were handled, duplicates were removed, and outliers were addressed.
Preprocessing: Column names were standardized, numerical columns were scaled, and skewness was reduced.
EDA: Visualizations like boxplots, histograms, and heatmaps were used to understand the data and its relationships.
Final Dataset: The dataset is now clean, consistent, and ready for further analysis or modeling.